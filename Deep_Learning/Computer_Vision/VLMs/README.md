# ğŸ§  Vision-Language Models (VLMs) with CLIP and Qwen2.5-VL

This repository showcases powerful **Vision-Language Models** combining image and text understanding, featuring **CLIP** and **Qwen2.5-VL** for tasks such as image captioning, object grounding, and semantic similarity.

---

## ğŸ“‚ Project Structure

ğŸ“ `Vision_Language_Models`

```
â”œâ”€â”€ .gitkeep
â”œâ”€â”€ CLIP Tokenization.ipynb
â”œâ”€â”€ Owen2.5 Zero Shot Object Detection.ipynb
â”œâ”€â”€ Owen 2.5 VLMs Image Captioning.ipynb
```

---

## ğŸ” Implementations Overview

### âš¡ CLIP Tokenization & Embedding

**Notebook**: `CLIP Tokenization.ipynb`
**Model**: `openai/clip-vit-base-patch32`

ğŸ“Œ **Highlights**:

* Tokenizes both images and text to generate embeddings
* Computes **cosine similarity** for image-image and text-text pairs
* Explores cross-modal relationships using `torch.nn.functional.cosine_similarity`
* Ideal for **zero-shot classification**, **semantic alignment**, and **embedding analysis**

---

### âš¡ Qwen2.5-VL Image Captioning

**Notebook**: `Owen 2.5 VLMs Image Captioning.ipynb`
**Model**: `Qwen/Qwen2.5-VL-3B-Instruct`

ğŸ“Œ **Highlights**:

* Accepts **multimodal prompts**: (Image + Text)
* Generates **zero-shot image captions** in natural language
* Uses HuggingFace Transformers + `qwen-vl-utils`
* Easy-to-use template-based chat formatting

---

### âš¡ Qwen2.5-VL Zero-Shot Object Detection

**Notebook**: `Owen2.5 Zero Shot Object Detection.ipynb`
**Model**: `Qwen/Qwen2.5-VL-3B-Instruct`

ğŸ“Œ **Highlights**:

* Performs **object grounding** using only visual input + descriptive prompt
* Outputs **JSON with bounding boxes** and **semantic labels**
* Perfect for **zero-shot localization** in unseen scenarios
* No training/fine-tuning required

---

## ğŸ§ª Requirements

Install dependencies using pip:

```bash
pip install torch torchvision transformers qwen-vl-utils matplotlib opencv-python
```

If you're using **Google Colab** with Hugging Face models:

```python
!pip install qwen-vl-utils

from huggingface_hub import login
login(token="your_huggingface_token")
```

---

## ğŸ’¡ Applications

âœ”ï¸ Zero-shot **image captioning**
âœ”ï¸ **Visual grounding** without retraining
âœ”ï¸ **Vision-language alignment** for embedding analysis
âœ”ï¸ Integration into **multimodal AI agents**, smart search, or chat-based systems

---

## ğŸ”— References

* [OpenAI CLIP Paper](https://arxiv.org/abs/2103.00020)
* [CLIP on Hugging Face](https://huggingface.co/openai/clip-vit-base-patch32)
* [Qwen2.5-VL Hugging Face Page](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
* [qwen-vl-utils GitHub](https://github.com/QwenLM/Qwen-VL)

---
